{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device selection + print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PyTorch utils\n",
    "\"\"\"\n",
    "\n",
    "import datetime\n",
    "import math\n",
    "import os\n",
    "import platform\n",
    "import subprocess\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def select_device(device='', batch_size=None, newline=True):\n",
    "    # device = 'cpu' or '0' or '0,1,2,3'\n",
    "    s = f'torch {torch.__version__} '  # string\n",
    "    device = str(device).strip().lower().replace('cuda:', '')  # to string, 'cuda:0' to '0'\n",
    "    cpu = device == 'cpu'\n",
    "    if cpu:\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # force torch.cuda.is_available() = False\n",
    "    elif device:  # non-cpu device requested\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = device  # set environment variable\n",
    "        assert torch.cuda.is_available(), f'CUDA unavailable, invalid device {device} requested'  # check availability\n",
    "\n",
    "    cuda = not cpu and torch.cuda.is_available()\n",
    "    if cuda:\n",
    "        devices = device.split(',') if device else '0'  # range(torch.cuda.device_count())  # i.e. 0,1,6,7\n",
    "        n = len(devices)  # device count\n",
    "        if n > 1 and batch_size:  # check batch_size is divisible by device_count\n",
    "            assert batch_size % n == 0, f'batch-size {batch_size} not multiple of GPU count {n}'\n",
    "        space = ' ' * (len(s) + 1)\n",
    "        for i, d in enumerate(devices):\n",
    "            p = torch.cuda.get_device_properties(i)\n",
    "            s += f\"{'' if i == 0 else space}CUDA:{d} ({p.name}, {p.total_memory / 1024 ** 2:.0f}MiB)\\n\"  # bytes to MB\n",
    "    else:\n",
    "        s += 'CPU\\n'\n",
    "\n",
    "    if not newline:\n",
    "        s = s.rstrip()\n",
    "    return torch.device('cuda:0' if cuda else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some info on running in colab + memory data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete (32 CPUs, 31.9 GB RAM, 1102.7/1863.0 GB disk)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def is_colab():\n",
    "    # Is environment a Google Colab instance?\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "def notebook_init(verbose=True):\n",
    "    # Check system software and hardware\n",
    "    print('Checking setup...')\n",
    "\n",
    "    import os\n",
    "    import shutil\n",
    "\n",
    "    import psutil\n",
    "    from IPython import display  # to display images and clear console output\n",
    "\n",
    "    if is_colab():\n",
    "        shutil.rmtree('/content/sample_data', ignore_errors=True)  # remove colab /sample_data directory\n",
    "\n",
    "    if verbose:\n",
    "        # System info\n",
    "        # gb = 1 / 1000 ** 3  # bytes to GB\n",
    "        gib = 1 / 1024 ** 3  # bytes to GiB\n",
    "        ram = psutil.virtual_memory().total\n",
    "        total, used, free = shutil.disk_usage(\"/\")\n",
    "        display.clear_output()\n",
    "        s = f'({os.cpu_count()} CPUs, {ram * gib:.1f} GB RAM, {(total - free) * gib:.1f}/{total * gib:.1f} GB disk)'\n",
    "    else:\n",
    "        s = ''\n",
    "\n",
    "    select_device(newline=False)\n",
    "    print(f'Setup complete {s}')\n",
    "    return display\n",
    "\n",
    "display=notebook_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.10.0+cpu\n",
      "Torchvision Version:  0.11.1+cpu\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import joblib as joblib\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top level data directory. Here we assume the format of the directory conforms\n",
    "#   to the ImageFolder structure\n",
    "data_dir = \"./data_torch\"\n",
    "\n",
    "# Models to choose from [resnet18, resnet34, alexnet, vgg, squeezenet, densenet]\n",
    "model_name = \"resnet34\"\n",
    "\n",
    "# Number of classes in the dataset\n",
    "num_classes = 100\n",
    "\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 32\n",
    "\n",
    "# Number of epochs to train for\n",
    "num_epochs = 300\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (5): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=100, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "    train_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "            else:\n",
    "                train_acc_history.append(epoch_acc.cpu().detach().item())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, train_acc_history, val_acc_history\n",
    "\n",
    "def initialize_model(model_name, num_classes, use_pretrained=True):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    #   variables is model specific.\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "\n",
    "    if model_name == \"resnet18\":\n",
    "        \"\"\" Resnet18\n",
    "        \"\"\"\n",
    "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"resnet34\":\n",
    "        \"\"\" Resnet34\n",
    "        \"\"\"\n",
    "        model_ft = models.resnet34(pretrained=use_pretrained)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"alexnet\":\n",
    "        \"\"\" Alexnet\n",
    "        \"\"\"\n",
    "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"vgg\":\n",
    "        \"\"\" VGG11_bn\n",
    "        \"\"\"\n",
    "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"squeezenet\":\n",
    "        \"\"\" Squeezenet\n",
    "        \"\"\"\n",
    "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
    "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "        model_ft.num_classes = num_classes\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"densenet\":\n",
    "        \"\"\" Densenet\n",
    "        \"\"\"\n",
    "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid model name, exiting...\")\n",
    "        exit()\n",
    "\n",
    "    return model_ft, input_size\n",
    "\n",
    "# Initialize the model for this run\n",
    "model_ft, input_size = initialize_model(model_name, num_classes, use_pretrained=True)\n",
    "\n",
    "# Print the model we just instantiated\n",
    "print(model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Datasets and Dataloaders...\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(input_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "print(\"Initializing Datasets and Dataloaders...\")\n",
    "\n",
    "# Create training and validation datasets\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}\n",
    "# Create training and validation dataloaders\n",
    "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t conv1.weight\n",
      "\t bn1.weight\n",
      "\t bn1.bias\n",
      "\t layer1.0.conv1.weight\n",
      "\t layer1.0.bn1.weight\n",
      "\t layer1.0.bn1.bias\n",
      "\t layer1.0.conv2.weight\n",
      "\t layer1.0.bn2.weight\n",
      "\t layer1.0.bn2.bias\n",
      "\t layer1.1.conv1.weight\n",
      "\t layer1.1.bn1.weight\n",
      "\t layer1.1.bn1.bias\n",
      "\t layer1.1.conv2.weight\n",
      "\t layer1.1.bn2.weight\n",
      "\t layer1.1.bn2.bias\n",
      "\t layer1.2.conv1.weight\n",
      "\t layer1.2.bn1.weight\n",
      "\t layer1.2.bn1.bias\n",
      "\t layer1.2.conv2.weight\n",
      "\t layer1.2.bn2.weight\n",
      "\t layer1.2.bn2.bias\n",
      "\t layer2.0.conv1.weight\n",
      "\t layer2.0.bn1.weight\n",
      "\t layer2.0.bn1.bias\n",
      "\t layer2.0.conv2.weight\n",
      "\t layer2.0.bn2.weight\n",
      "\t layer2.0.bn2.bias\n",
      "\t layer2.0.downsample.0.weight\n",
      "\t layer2.0.downsample.1.weight\n",
      "\t layer2.0.downsample.1.bias\n",
      "\t layer2.1.conv1.weight\n",
      "\t layer2.1.bn1.weight\n",
      "\t layer2.1.bn1.bias\n",
      "\t layer2.1.conv2.weight\n",
      "\t layer2.1.bn2.weight\n",
      "\t layer2.1.bn2.bias\n",
      "\t layer2.2.conv1.weight\n",
      "\t layer2.2.bn1.weight\n",
      "\t layer2.2.bn1.bias\n",
      "\t layer2.2.conv2.weight\n",
      "\t layer2.2.bn2.weight\n",
      "\t layer2.2.bn2.bias\n",
      "\t layer2.3.conv1.weight\n",
      "\t layer2.3.bn1.weight\n",
      "\t layer2.3.bn1.bias\n",
      "\t layer2.3.conv2.weight\n",
      "\t layer2.3.bn2.weight\n",
      "\t layer2.3.bn2.bias\n",
      "\t layer3.0.conv1.weight\n",
      "\t layer3.0.bn1.weight\n",
      "\t layer3.0.bn1.bias\n",
      "\t layer3.0.conv2.weight\n",
      "\t layer3.0.bn2.weight\n",
      "\t layer3.0.bn2.bias\n",
      "\t layer3.0.downsample.0.weight\n",
      "\t layer3.0.downsample.1.weight\n",
      "\t layer3.0.downsample.1.bias\n",
      "\t layer3.1.conv1.weight\n",
      "\t layer3.1.bn1.weight\n",
      "\t layer3.1.bn1.bias\n",
      "\t layer3.1.conv2.weight\n",
      "\t layer3.1.bn2.weight\n",
      "\t layer3.1.bn2.bias\n",
      "\t layer3.2.conv1.weight\n",
      "\t layer3.2.bn1.weight\n",
      "\t layer3.2.bn1.bias\n",
      "\t layer3.2.conv2.weight\n",
      "\t layer3.2.bn2.weight\n",
      "\t layer3.2.bn2.bias\n",
      "\t layer3.3.conv1.weight\n",
      "\t layer3.3.bn1.weight\n",
      "\t layer3.3.bn1.bias\n",
      "\t layer3.3.conv2.weight\n",
      "\t layer3.3.bn2.weight\n",
      "\t layer3.3.bn2.bias\n",
      "\t layer3.4.conv1.weight\n",
      "\t layer3.4.bn1.weight\n",
      "\t layer3.4.bn1.bias\n",
      "\t layer3.4.conv2.weight\n",
      "\t layer3.4.bn2.weight\n",
      "\t layer3.4.bn2.bias\n",
      "\t layer3.5.conv1.weight\n",
      "\t layer3.5.bn1.weight\n",
      "\t layer3.5.bn1.bias\n",
      "\t layer3.5.conv2.weight\n",
      "\t layer3.5.bn2.weight\n",
      "\t layer3.5.bn2.bias\n",
      "\t layer4.0.conv1.weight\n",
      "\t layer4.0.bn1.weight\n",
      "\t layer4.0.bn1.bias\n",
      "\t layer4.0.conv2.weight\n",
      "\t layer4.0.bn2.weight\n",
      "\t layer4.0.bn2.bias\n",
      "\t layer4.0.downsample.0.weight\n",
      "\t layer4.0.downsample.1.weight\n",
      "\t layer4.0.downsample.1.bias\n",
      "\t layer4.1.conv1.weight\n",
      "\t layer4.1.bn1.weight\n",
      "\t layer4.1.bn1.bias\n",
      "\t layer4.1.conv2.weight\n",
      "\t layer4.1.bn2.weight\n",
      "\t layer4.1.bn2.bias\n",
      "\t layer4.2.conv1.weight\n",
      "\t layer4.2.bn1.weight\n",
      "\t layer4.2.bn1.bias\n",
      "\t layer4.2.conv2.weight\n",
      "\t layer4.2.bn2.weight\n",
      "\t layer4.2.bn2.bias\n",
      "\t fc.weight\n",
      "\t fc.bias\n"
     ]
    }
   ],
   "source": [
    "# Send the model to GPU\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# Gather the parameters to be optimized/updated in this run. If we are\n",
    "#  finetuning we will be updating all parameters. However, if we are\n",
    "#  doing feature extract method, we will only update the parameters\n",
    "#  that we have just initialized, i.e. the parameters with requires_grad\n",
    "#  is True.\n",
    "params_to_update = model_ft.parameters()\n",
    "print(\"Params to learn:\")\n",
    "for name,param in model_ft.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        print(\"\\t\",name)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "learning_rate = 0.001\n",
    "optimizer_ft = optim.Adam(params_to_update, lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/299\n",
      "----------\n",
      "train Loss: 5.0272 Acc: 0.0037\n",
      "val Loss: 19.6261 Acc: 0.0000\n",
      "\n",
      "Epoch 1/299\n",
      "----------\n",
      "train Loss: 4.7138 Acc: 0.0147\n",
      "val Loss: 4.8242 Acc: 0.0250\n",
      "\n",
      "Epoch 2/299\n",
      "----------\n",
      "train Loss: 4.6781 Acc: 0.0128\n",
      "val Loss: 8.3813 Acc: 0.0150\n",
      "\n",
      "Epoch 3/299\n",
      "----------\n",
      "train Loss: 4.6163 Acc: 0.0147\n",
      "val Loss: 5.0227 Acc: 0.0150\n",
      "\n",
      "Epoch 4/299\n",
      "----------\n",
      "train Loss: 4.5796 Acc: 0.0165\n",
      "val Loss: 4.7140 Acc: 0.0200\n",
      "\n",
      "Epoch 5/299\n",
      "----------\n",
      "train Loss: 4.5138 Acc: 0.0147\n",
      "val Loss: 4.5886 Acc: 0.0050\n",
      "\n",
      "Epoch 6/299\n",
      "----------\n",
      "train Loss: 4.4717 Acc: 0.0293\n",
      "val Loss: 4.6632 Acc: 0.0100\n",
      "\n",
      "Epoch 7/299\n",
      "----------\n",
      "train Loss: 4.4919 Acc: 0.0110\n",
      "val Loss: 4.6067 Acc: 0.0100\n",
      "\n",
      "Epoch 8/299\n",
      "----------\n",
      "train Loss: 4.4422 Acc: 0.0220\n",
      "val Loss: 4.6454 Acc: 0.0300\n",
      "\n",
      "Epoch 9/299\n",
      "----------\n",
      "train Loss: 4.4259 Acc: 0.0256\n",
      "val Loss: 5.0699 Acc: 0.0200\n",
      "\n",
      "Epoch 10/299\n",
      "----------\n",
      "train Loss: 4.3251 Acc: 0.0256\n",
      "val Loss: 4.5902 Acc: 0.0000\n",
      "\n",
      "Epoch 11/299\n",
      "----------\n",
      "train Loss: 4.3291 Acc: 0.0366\n",
      "val Loss: 4.6921 Acc: 0.0150\n",
      "\n",
      "Epoch 12/299\n",
      "----------\n",
      "train Loss: 4.2793 Acc: 0.0275\n",
      "val Loss: 5.0769 Acc: 0.0050\n",
      "\n",
      "Epoch 13/299\n",
      "----------\n",
      "train Loss: 4.2533 Acc: 0.0256\n",
      "val Loss: 4.8131 Acc: 0.0200\n",
      "\n",
      "Epoch 14/299\n",
      "----------\n",
      "train Loss: 4.2612 Acc: 0.0440\n",
      "val Loss: 4.6742 Acc: 0.0300\n",
      "\n",
      "Epoch 15/299\n",
      "----------\n",
      "train Loss: 4.1682 Acc: 0.0421\n",
      "val Loss: 4.8756 Acc: 0.0200\n",
      "\n",
      "Epoch 16/299\n",
      "----------\n",
      "train Loss: 4.1456 Acc: 0.0421\n",
      "val Loss: 4.7291 Acc: 0.0150\n",
      "\n",
      "Epoch 17/299\n",
      "----------\n",
      "train Loss: 4.1389 Acc: 0.0531\n",
      "val Loss: 6.1623 Acc: 0.0150\n",
      "\n",
      "Epoch 18/299\n",
      "----------\n",
      "train Loss: 4.1042 Acc: 0.0549\n",
      "val Loss: 4.6433 Acc: 0.0200\n",
      "\n",
      "Epoch 19/299\n",
      "----------\n",
      "train Loss: 4.0144 Acc: 0.0495\n",
      "val Loss: 4.8967 Acc: 0.0200\n",
      "\n",
      "Epoch 20/299\n",
      "----------\n",
      "train Loss: 4.0108 Acc: 0.0751\n",
      "val Loss: 5.1895 Acc: 0.0150\n",
      "\n",
      "Epoch 21/299\n",
      "----------\n",
      "train Loss: 4.0066 Acc: 0.0641\n",
      "val Loss: 4.8708 Acc: 0.0250\n",
      "\n",
      "Epoch 22/299\n",
      "----------\n",
      "train Loss: 3.9143 Acc: 0.0549\n",
      "val Loss: 4.7580 Acc: 0.0250\n",
      "\n",
      "Epoch 23/299\n",
      "----------\n",
      "train Loss: 3.8811 Acc: 0.0659\n",
      "val Loss: 4.9214 Acc: 0.0400\n",
      "\n",
      "Epoch 24/299\n",
      "----------\n",
      "train Loss: 3.9521 Acc: 0.0751\n",
      "val Loss: 5.1408 Acc: 0.0350\n",
      "\n",
      "Epoch 25/299\n",
      "----------\n",
      "train Loss: 3.8250 Acc: 0.0769\n",
      "val Loss: 5.2021 Acc: 0.0300\n",
      "\n",
      "Epoch 26/299\n",
      "----------\n",
      "train Loss: 3.8631 Acc: 0.0842\n",
      "val Loss: 5.1800 Acc: 0.0300\n",
      "\n",
      "Epoch 27/299\n",
      "----------\n",
      "train Loss: 3.8251 Acc: 0.0824\n",
      "val Loss: 4.8146 Acc: 0.0300\n",
      "\n",
      "Epoch 28/299\n",
      "----------\n",
      "train Loss: 3.6941 Acc: 0.1099\n",
      "val Loss: 5.0623 Acc: 0.0250\n",
      "\n",
      "Epoch 29/299\n",
      "----------\n",
      "train Loss: 3.6745 Acc: 0.0861\n",
      "val Loss: 5.0025 Acc: 0.0250\n",
      "\n",
      "Epoch 30/299\n",
      "----------\n",
      "train Loss: 3.7097 Acc: 0.0989\n",
      "val Loss: 4.9307 Acc: 0.0500\n",
      "\n",
      "Epoch 31/299\n",
      "----------\n",
      "train Loss: 3.6208 Acc: 0.1154\n",
      "val Loss: 4.9110 Acc: 0.0500\n",
      "\n",
      "Epoch 32/299\n",
      "----------\n",
      "train Loss: 3.6300 Acc: 0.1154\n",
      "val Loss: 5.0088 Acc: 0.0300\n",
      "\n",
      "Epoch 33/299\n",
      "----------\n",
      "train Loss: 3.6333 Acc: 0.1081\n",
      "val Loss: 5.1214 Acc: 0.0300\n",
      "\n",
      "Epoch 34/299\n",
      "----------\n",
      "train Loss: 3.5973 Acc: 0.0989\n",
      "val Loss: 4.9329 Acc: 0.0500\n",
      "\n",
      "Epoch 35/299\n",
      "----------\n",
      "train Loss: 3.4806 Acc: 0.1282\n",
      "val Loss: 5.1048 Acc: 0.0650\n",
      "\n",
      "Epoch 36/299\n",
      "----------\n",
      "train Loss: 3.4782 Acc: 0.1154\n",
      "val Loss: 5.4114 Acc: 0.0300\n",
      "\n",
      "Epoch 37/299\n",
      "----------\n",
      "train Loss: 3.3840 Acc: 0.1410\n",
      "val Loss: 5.3441 Acc: 0.0350\n",
      "\n",
      "Epoch 38/299\n",
      "----------\n",
      "train Loss: 3.3575 Acc: 0.1575\n",
      "val Loss: 5.4907 Acc: 0.0250\n",
      "\n",
      "Epoch 39/299\n",
      "----------\n",
      "train Loss: 3.4033 Acc: 0.1410\n",
      "val Loss: 5.1529 Acc: 0.0450\n",
      "\n",
      "Epoch 40/299\n",
      "----------\n",
      "train Loss: 3.3899 Acc: 0.1612\n",
      "val Loss: 5.2864 Acc: 0.0450\n",
      "\n",
      "Epoch 41/299\n",
      "----------\n",
      "train Loss: 3.3556 Acc: 0.1502\n",
      "val Loss: 5.3503 Acc: 0.0550\n",
      "\n",
      "Epoch 42/299\n",
      "----------\n",
      "train Loss: 3.2835 Acc: 0.1575\n",
      "val Loss: 5.6865 Acc: 0.0350\n",
      "\n",
      "Epoch 43/299\n",
      "----------\n",
      "train Loss: 3.3311 Acc: 0.1520\n",
      "val Loss: 5.3917 Acc: 0.0450\n",
      "\n",
      "Epoch 44/299\n",
      "----------\n",
      "train Loss: 3.1802 Acc: 0.1575\n",
      "val Loss: 5.6267 Acc: 0.0350\n",
      "\n",
      "Epoch 45/299\n",
      "----------\n",
      "train Loss: 3.1298 Acc: 0.1703\n",
      "val Loss: 5.4328 Acc: 0.0750\n",
      "\n",
      "Epoch 46/299\n",
      "----------\n",
      "train Loss: 3.2382 Acc: 0.1758\n",
      "val Loss: 5.5750 Acc: 0.0450\n",
      "\n",
      "Epoch 47/299\n",
      "----------\n",
      "train Loss: 3.1035 Acc: 0.1795\n",
      "val Loss: 5.4080 Acc: 0.0650\n",
      "\n",
      "Epoch 48/299\n",
      "----------\n",
      "train Loss: 2.9699 Acc: 0.1960\n",
      "val Loss: 5.6232 Acc: 0.0350\n",
      "\n",
      "Epoch 49/299\n",
      "----------\n",
      "train Loss: 3.0495 Acc: 0.2070\n",
      "val Loss: 5.4666 Acc: 0.0250\n",
      "\n",
      "Epoch 50/299\n",
      "----------\n",
      "train Loss: 3.0121 Acc: 0.2070\n",
      "val Loss: 5.6366 Acc: 0.0450\n",
      "\n",
      "Epoch 51/299\n",
      "----------\n",
      "train Loss: 3.0105 Acc: 0.1996\n",
      "val Loss: 5.3725 Acc: 0.0550\n",
      "\n",
      "Epoch 52/299\n",
      "----------\n",
      "train Loss: 3.0227 Acc: 0.1868\n",
      "val Loss: 5.6591 Acc: 0.0400\n",
      "\n",
      "Epoch 53/299\n",
      "----------\n",
      "train Loss: 2.9002 Acc: 0.2436\n",
      "val Loss: 5.8978 Acc: 0.0250\n",
      "\n",
      "Epoch 54/299\n",
      "----------\n",
      "train Loss: 2.7615 Acc: 0.2564\n",
      "val Loss: 5.7485 Acc: 0.0550\n",
      "\n",
      "Epoch 55/299\n",
      "----------\n",
      "train Loss: 2.7846 Acc: 0.2747\n",
      "val Loss: 6.1300 Acc: 0.0300\n",
      "\n",
      "Epoch 56/299\n",
      "----------\n",
      "train Loss: 2.8891 Acc: 0.2399\n",
      "val Loss: 5.6879 Acc: 0.0550\n",
      "\n",
      "Epoch 57/299\n",
      "----------\n",
      "train Loss: 2.8449 Acc: 0.2216\n",
      "val Loss: 6.2623 Acc: 0.0550\n",
      "\n",
      "Epoch 58/299\n",
      "----------\n",
      "train Loss: 2.7409 Acc: 0.2674\n",
      "val Loss: 5.8302 Acc: 0.0650\n",
      "\n",
      "Epoch 59/299\n",
      "----------\n",
      "train Loss: 2.8088 Acc: 0.2509\n",
      "val Loss: 6.4701 Acc: 0.0500\n",
      "\n",
      "Epoch 60/299\n",
      "----------\n",
      "train Loss: 2.7533 Acc: 0.2473\n",
      "val Loss: 6.2387 Acc: 0.0350\n",
      "\n",
      "Epoch 61/299\n",
      "----------\n",
      "train Loss: 2.6509 Acc: 0.2949\n",
      "val Loss: 6.7120 Acc: 0.0400\n",
      "\n",
      "Epoch 62/299\n",
      "----------\n",
      "train Loss: 2.6947 Acc: 0.2912\n",
      "val Loss: 6.2214 Acc: 0.0700\n",
      "\n",
      "Epoch 63/299\n",
      "----------\n",
      "train Loss: 2.5686 Acc: 0.2784\n",
      "val Loss: 5.8031 Acc: 0.0550\n",
      "\n",
      "Epoch 64/299\n",
      "----------\n",
      "train Loss: 2.4705 Acc: 0.3297\n",
      "val Loss: 6.0409 Acc: 0.0400\n",
      "\n",
      "Epoch 65/299\n",
      "----------\n",
      "train Loss: 2.5278 Acc: 0.3205\n",
      "val Loss: 5.8468 Acc: 0.0600\n",
      "\n",
      "Epoch 66/299\n",
      "----------\n",
      "train Loss: 2.4645 Acc: 0.3260\n",
      "val Loss: 6.4602 Acc: 0.0550\n",
      "\n",
      "Epoch 67/299\n",
      "----------\n",
      "train Loss: 2.6123 Acc: 0.2839\n",
      "val Loss: 6.0860 Acc: 0.0500\n",
      "\n",
      "Epoch 68/299\n",
      "----------\n",
      "train Loss: 2.4990 Acc: 0.3315\n",
      "val Loss: 6.2234 Acc: 0.0750\n",
      "\n",
      "Epoch 69/299\n",
      "----------\n",
      "train Loss: 2.4075 Acc: 0.3516\n",
      "val Loss: 6.4151 Acc: 0.0550\n",
      "\n",
      "Epoch 70/299\n",
      "----------\n",
      "train Loss: 2.3859 Acc: 0.3388\n",
      "val Loss: 6.5195 Acc: 0.0450\n",
      "\n",
      "Epoch 71/299\n",
      "----------\n",
      "train Loss: 2.4068 Acc: 0.3425\n",
      "val Loss: 6.4295 Acc: 0.0650\n",
      "\n",
      "Epoch 72/299\n",
      "----------\n",
      "train Loss: 2.3052 Acc: 0.3736\n",
      "val Loss: 6.5957 Acc: 0.0450\n",
      "\n",
      "Epoch 73/299\n",
      "----------\n",
      "train Loss: 2.3621 Acc: 0.3663\n",
      "val Loss: 6.5577 Acc: 0.0450\n",
      "\n",
      "Epoch 74/299\n",
      "----------\n",
      "train Loss: 2.3087 Acc: 0.3810\n",
      "val Loss: 6.3215 Acc: 0.0400\n",
      "\n",
      "Epoch 75/299\n",
      "----------\n",
      "train Loss: 2.2955 Acc: 0.3828\n",
      "val Loss: 6.0369 Acc: 0.0750\n",
      "\n",
      "Epoch 76/299\n",
      "----------\n",
      "train Loss: 2.0999 Acc: 0.4139\n",
      "val Loss: 7.9035 Acc: 0.0600\n",
      "\n",
      "Epoch 77/299\n",
      "----------\n",
      "train Loss: 2.1179 Acc: 0.4066\n",
      "val Loss: 6.5453 Acc: 0.0600\n",
      "\n",
      "Epoch 78/299\n",
      "----------\n",
      "train Loss: 2.0570 Acc: 0.4359\n",
      "val Loss: 6.4276 Acc: 0.0800\n",
      "\n",
      "Epoch 79/299\n",
      "----------\n",
      "train Loss: 2.0647 Acc: 0.4249\n",
      "val Loss: 6.1173 Acc: 0.0850\n",
      "\n",
      "Epoch 80/299\n",
      "----------\n",
      "train Loss: 2.1740 Acc: 0.4084\n",
      "val Loss: 6.2185 Acc: 0.0950\n",
      "\n",
      "Epoch 81/299\n",
      "----------\n",
      "train Loss: 2.1011 Acc: 0.4103\n",
      "val Loss: 6.5740 Acc: 0.0750\n",
      "\n",
      "Epoch 82/299\n",
      "----------\n",
      "train Loss: 2.1913 Acc: 0.4139\n",
      "val Loss: 6.6418 Acc: 0.0600\n",
      "\n",
      "Epoch 83/299\n",
      "----------\n",
      "train Loss: 2.0073 Acc: 0.4377\n",
      "val Loss: 6.0478 Acc: 0.0650\n",
      "\n",
      "Epoch 84/299\n",
      "----------\n",
      "train Loss: 2.0157 Acc: 0.4451\n",
      "val Loss: 7.2551 Acc: 0.0400\n",
      "\n",
      "Epoch 85/299\n",
      "----------\n",
      "train Loss: 1.8690 Acc: 0.4762\n",
      "val Loss: 6.8037 Acc: 0.0650\n",
      "\n",
      "Epoch 86/299\n",
      "----------\n",
      "train Loss: 1.9319 Acc: 0.4725\n",
      "val Loss: 6.6419 Acc: 0.0650\n",
      "\n",
      "Epoch 87/299\n",
      "----------\n",
      "train Loss: 1.8618 Acc: 0.4835\n",
      "val Loss: 6.1988 Acc: 0.0700\n",
      "\n",
      "Epoch 88/299\n",
      "----------\n",
      "train Loss: 1.8652 Acc: 0.4780\n",
      "val Loss: 6.7300 Acc: 0.0700\n",
      "\n",
      "Epoch 89/299\n",
      "----------\n",
      "train Loss: 1.9405 Acc: 0.4451\n",
      "val Loss: 7.0604 Acc: 0.0800\n",
      "\n",
      "Epoch 90/299\n",
      "----------\n",
      "train Loss: 1.8860 Acc: 0.4799\n",
      "val Loss: 7.0435 Acc: 0.0650\n",
      "\n",
      "Epoch 91/299\n",
      "----------\n",
      "train Loss: 1.8217 Acc: 0.4670\n",
      "val Loss: 6.4832 Acc: 0.0750\n",
      "\n",
      "Epoch 92/299\n",
      "----------\n",
      "train Loss: 1.6551 Acc: 0.5586\n",
      "val Loss: 6.5207 Acc: 0.0750\n",
      "\n",
      "Epoch 93/299\n",
      "----------\n",
      "train Loss: 1.8149 Acc: 0.4945\n",
      "val Loss: 6.7804 Acc: 0.0800\n",
      "\n",
      "Epoch 94/299\n",
      "----------\n",
      "train Loss: 1.7409 Acc: 0.5055\n",
      "val Loss: 6.6678 Acc: 0.0600\n",
      "\n",
      "Epoch 95/299\n",
      "----------\n",
      "train Loss: 1.8098 Acc: 0.5330\n",
      "val Loss: 6.6042 Acc: 0.0550\n",
      "\n",
      "Epoch 96/299\n",
      "----------\n",
      "train Loss: 1.8846 Acc: 0.4853\n",
      "val Loss: 6.6470 Acc: 0.0650\n",
      "\n",
      "Epoch 97/299\n",
      "----------\n",
      "train Loss: 1.7431 Acc: 0.5165\n",
      "val Loss: 6.9430 Acc: 0.0650\n",
      "\n",
      "Epoch 98/299\n",
      "----------\n",
      "train Loss: 1.7228 Acc: 0.5201\n",
      "val Loss: 7.0233 Acc: 0.0750\n",
      "\n",
      "Epoch 99/299\n",
      "----------\n",
      "train Loss: 1.6445 Acc: 0.5330\n",
      "val Loss: 7.9998 Acc: 0.0650\n",
      "\n",
      "Epoch 100/299\n",
      "----------\n",
      "train Loss: 1.7535 Acc: 0.5330\n",
      "val Loss: 6.9156 Acc: 0.0800\n",
      "\n",
      "Epoch 101/299\n",
      "----------\n",
      "train Loss: 1.7136 Acc: 0.5330\n",
      "val Loss: 6.5894 Acc: 0.0650\n",
      "\n",
      "Epoch 102/299\n",
      "----------\n",
      "train Loss: 1.5979 Acc: 0.5769\n",
      "val Loss: 7.2156 Acc: 0.0550\n",
      "\n",
      "Epoch 103/299\n",
      "----------\n",
      "train Loss: 1.6088 Acc: 0.5440\n",
      "val Loss: 6.7910 Acc: 0.0900\n",
      "\n",
      "Epoch 104/299\n",
      "----------\n",
      "train Loss: 1.5387 Acc: 0.5916\n",
      "val Loss: 7.0858 Acc: 0.0700\n",
      "\n",
      "Epoch 105/299\n",
      "----------\n",
      "train Loss: 1.5293 Acc: 0.5678\n",
      "val Loss: 7.0089 Acc: 0.0800\n",
      "\n",
      "Epoch 106/299\n",
      "----------\n",
      "train Loss: 1.4941 Acc: 0.5824\n",
      "val Loss: 6.6150 Acc: 0.0850\n",
      "\n",
      "Epoch 107/299\n",
      "----------\n",
      "train Loss: 1.4589 Acc: 0.6227\n",
      "val Loss: 7.2292 Acc: 0.0650\n",
      "\n",
      "Epoch 108/299\n",
      "----------\n",
      "train Loss: 1.4078 Acc: 0.6154\n",
      "val Loss: 7.6518 Acc: 0.0650\n",
      "\n",
      "Epoch 109/299\n",
      "----------\n",
      "train Loss: 1.5029 Acc: 0.5788\n",
      "val Loss: 6.5740 Acc: 0.0800\n",
      "\n",
      "Epoch 110/299\n",
      "----------\n",
      "train Loss: 1.4756 Acc: 0.5934\n",
      "val Loss: 6.9934 Acc: 0.0750\n",
      "\n",
      "Epoch 111/299\n",
      "----------\n",
      "train Loss: 1.4434 Acc: 0.6099\n",
      "val Loss: 7.2594 Acc: 0.0800\n",
      "\n",
      "Epoch 112/299\n",
      "----------\n",
      "train Loss: 1.4987 Acc: 0.5788\n",
      "val Loss: 6.8686 Acc: 0.0750\n",
      "\n",
      "Epoch 113/299\n",
      "----------\n",
      "train Loss: 1.4220 Acc: 0.6245\n",
      "val Loss: 7.0304 Acc: 0.0750\n",
      "\n",
      "Epoch 114/299\n",
      "----------\n",
      "train Loss: 1.3288 Acc: 0.6227\n",
      "val Loss: 7.0038 Acc: 0.0550\n",
      "\n",
      "Epoch 115/299\n",
      "----------\n",
      "train Loss: 1.3514 Acc: 0.6245\n",
      "val Loss: 6.9194 Acc: 0.0600\n",
      "\n",
      "Epoch 116/299\n",
      "----------\n",
      "train Loss: 1.2576 Acc: 0.6630\n",
      "val Loss: 6.9627 Acc: 0.0800\n",
      "\n",
      "Epoch 117/299\n",
      "----------\n",
      "train Loss: 1.2726 Acc: 0.6447\n",
      "val Loss: 7.1422 Acc: 0.0850\n",
      "\n",
      "Epoch 118/299\n",
      "----------\n",
      "train Loss: 1.2289 Acc: 0.6447\n",
      "val Loss: 7.2574 Acc: 0.0550\n",
      "\n",
      "Epoch 119/299\n",
      "----------\n",
      "train Loss: 1.2615 Acc: 0.6392\n",
      "val Loss: 7.3835 Acc: 0.0750\n",
      "\n",
      "Epoch 120/299\n",
      "----------\n",
      "train Loss: 1.2577 Acc: 0.6593\n",
      "val Loss: 7.2698 Acc: 0.0750\n",
      "\n",
      "Epoch 121/299\n",
      "----------\n",
      "train Loss: 1.2351 Acc: 0.6795\n",
      "val Loss: 6.8679 Acc: 0.0700\n",
      "\n",
      "Epoch 122/299\n",
      "----------\n",
      "train Loss: 1.2994 Acc: 0.6209\n",
      "val Loss: 7.6149 Acc: 0.0550\n",
      "\n",
      "Epoch 123/299\n",
      "----------\n",
      "train Loss: 1.2882 Acc: 0.6190\n",
      "val Loss: 7.5071 Acc: 0.0650\n",
      "\n",
      "Epoch 124/299\n",
      "----------\n",
      "train Loss: 1.3796 Acc: 0.6044\n",
      "val Loss: 7.1189 Acc: 0.0950\n",
      "\n",
      "Epoch 125/299\n",
      "----------\n",
      "train Loss: 1.2678 Acc: 0.6282\n",
      "val Loss: 7.4499 Acc: 0.0800\n",
      "\n",
      "Epoch 126/299\n",
      "----------\n",
      "train Loss: 1.3297 Acc: 0.6227\n",
      "val Loss: 7.4395 Acc: 0.0750\n",
      "\n",
      "Epoch 127/299\n",
      "----------\n",
      "train Loss: 1.3241 Acc: 0.6484\n",
      "val Loss: 7.3924 Acc: 0.0550\n",
      "\n",
      "Epoch 128/299\n",
      "----------\n",
      "train Loss: 1.2819 Acc: 0.6355\n",
      "val Loss: 7.5390 Acc: 0.0650\n",
      "\n",
      "Epoch 129/299\n",
      "----------\n",
      "train Loss: 1.2751 Acc: 0.6502\n",
      "val Loss: 7.3047 Acc: 0.0700\n",
      "\n",
      "Epoch 130/299\n",
      "----------\n",
      "train Loss: 1.2320 Acc: 0.6355\n",
      "val Loss: 7.0841 Acc: 0.0800\n",
      "\n",
      "Epoch 131/299\n",
      "----------\n",
      "train Loss: 1.0841 Acc: 0.6905\n",
      "val Loss: 7.8403 Acc: 0.0500\n",
      "\n",
      "Epoch 132/299\n",
      "----------\n",
      "train Loss: 1.1642 Acc: 0.6685\n",
      "val Loss: 7.4756 Acc: 0.0700\n",
      "\n",
      "Epoch 133/299\n",
      "----------\n",
      "train Loss: 1.2801 Acc: 0.6648\n",
      "val Loss: 7.4313 Acc: 0.0800\n",
      "\n",
      "Epoch 134/299\n",
      "----------\n",
      "train Loss: 1.0315 Acc: 0.7051\n",
      "val Loss: 7.5276 Acc: 0.0550\n",
      "\n",
      "Epoch 135/299\n",
      "----------\n",
      "train Loss: 1.1256 Acc: 0.7198\n",
      "val Loss: 7.3389 Acc: 0.0750\n",
      "\n",
      "Epoch 136/299\n",
      "----------\n",
      "train Loss: 1.0536 Acc: 0.6978\n",
      "val Loss: 7.9439 Acc: 0.0550\n",
      "\n",
      "Epoch 137/299\n",
      "----------\n",
      "train Loss: 1.1726 Acc: 0.6832\n",
      "val Loss: 7.3571 Acc: 0.0750\n",
      "\n",
      "Epoch 138/299\n",
      "----------\n",
      "train Loss: 1.0315 Acc: 0.7234\n",
      "val Loss: 7.2211 Acc: 0.0800\n",
      "\n",
      "Epoch 139/299\n",
      "----------\n",
      "train Loss: 1.0215 Acc: 0.7143\n",
      "val Loss: 7.5881 Acc: 0.0600\n",
      "\n",
      "Epoch 140/299\n",
      "----------\n",
      "train Loss: 1.1984 Acc: 0.6703\n",
      "val Loss: 7.7404 Acc: 0.0650\n",
      "\n",
      "Epoch 141/299\n",
      "----------\n",
      "train Loss: 1.0515 Acc: 0.7106\n",
      "val Loss: 8.0680 Acc: 0.0650\n",
      "\n",
      "Epoch 142/299\n",
      "----------\n",
      "train Loss: 0.9687 Acc: 0.7381\n",
      "val Loss: 7.4582 Acc: 0.0700\n",
      "\n",
      "Epoch 143/299\n",
      "----------\n",
      "train Loss: 0.9142 Acc: 0.7619\n",
      "val Loss: 7.2085 Acc: 0.0800\n",
      "\n",
      "Epoch 144/299\n",
      "----------\n",
      "train Loss: 1.1369 Acc: 0.6832\n",
      "val Loss: 7.4910 Acc: 0.0800\n",
      "\n",
      "Epoch 145/299\n",
      "----------\n",
      "train Loss: 1.0367 Acc: 0.7363\n",
      "val Loss: 7.1305 Acc: 0.0650\n",
      "\n",
      "Epoch 146/299\n",
      "----------\n",
      "train Loss: 0.9087 Acc: 0.7454\n",
      "val Loss: 7.2462 Acc: 0.0850\n",
      "\n",
      "Epoch 147/299\n",
      "----------\n",
      "train Loss: 0.8828 Acc: 0.7766\n",
      "val Loss: 7.2051 Acc: 0.0950\n",
      "\n",
      "Epoch 148/299\n",
      "----------\n",
      "train Loss: 0.9170 Acc: 0.7729\n",
      "val Loss: 7.7283 Acc: 0.0850\n",
      "\n",
      "Epoch 149/299\n",
      "----------\n",
      "train Loss: 1.0721 Acc: 0.6996\n",
      "val Loss: 7.5400 Acc: 0.0750\n",
      "\n",
      "Epoch 150/299\n",
      "----------\n",
      "train Loss: 0.9050 Acc: 0.7527\n",
      "val Loss: 7.9091 Acc: 0.0500\n",
      "\n",
      "Epoch 151/299\n",
      "----------\n",
      "train Loss: 1.0648 Acc: 0.7143\n",
      "val Loss: 7.7189 Acc: 0.0800\n",
      "\n",
      "Epoch 152/299\n",
      "----------\n",
      "train Loss: 1.0097 Acc: 0.7381\n",
      "val Loss: 7.5261 Acc: 0.0650\n",
      "\n",
      "Epoch 153/299\n",
      "----------\n",
      "train Loss: 1.0166 Acc: 0.7070\n",
      "val Loss: 7.8126 Acc: 0.0600\n",
      "\n",
      "Epoch 154/299\n",
      "----------\n",
      "train Loss: 1.0165 Acc: 0.7308\n",
      "val Loss: 7.5628 Acc: 0.0700\n",
      "\n",
      "Epoch 155/299\n",
      "----------\n",
      "train Loss: 0.9527 Acc: 0.7198\n",
      "val Loss: 7.7914 Acc: 0.0650\n",
      "\n",
      "Epoch 156/299\n",
      "----------\n",
      "train Loss: 1.0851 Acc: 0.6923\n",
      "val Loss: 7.8592 Acc: 0.0550\n",
      "\n",
      "Epoch 157/299\n",
      "----------\n",
      "train Loss: 0.8749 Acc: 0.7509\n",
      "val Loss: 7.1504 Acc: 0.0850\n",
      "\n",
      "Epoch 158/299\n",
      "----------\n",
      "train Loss: 0.8986 Acc: 0.7527\n",
      "val Loss: 7.3084 Acc: 0.0750\n",
      "\n",
      "Epoch 159/299\n",
      "----------\n",
      "train Loss: 0.9732 Acc: 0.7198\n",
      "val Loss: 7.8478 Acc: 0.0650\n",
      "\n",
      "Epoch 160/299\n",
      "----------\n",
      "train Loss: 0.9658 Acc: 0.7234\n",
      "val Loss: 7.3243 Acc: 0.0700\n",
      "\n",
      "Epoch 161/299\n",
      "----------\n",
      "train Loss: 0.9828 Acc: 0.7418\n",
      "val Loss: 7.8252 Acc: 0.0750\n",
      "\n",
      "Epoch 162/299\n",
      "----------\n",
      "train Loss: 0.9199 Acc: 0.7418\n",
      "val Loss: 7.6475 Acc: 0.0600\n",
      "\n",
      "Epoch 163/299\n",
      "----------\n",
      "train Loss: 0.9053 Acc: 0.7601\n",
      "val Loss: 7.3814 Acc: 0.0800\n",
      "\n",
      "Epoch 164/299\n",
      "----------\n",
      "train Loss: 1.0453 Acc: 0.7308\n",
      "val Loss: 8.1261 Acc: 0.0650\n",
      "\n",
      "Epoch 165/299\n",
      "----------\n",
      "train Loss: 0.9123 Acc: 0.7564\n",
      "val Loss: 7.5637 Acc: 0.0900\n",
      "\n",
      "Epoch 166/299\n",
      "----------\n",
      "train Loss: 1.0651 Acc: 0.6758\n",
      "val Loss: 7.6132 Acc: 0.0700\n",
      "\n",
      "Epoch 167/299\n",
      "----------\n",
      "train Loss: 0.8955 Acc: 0.7363\n",
      "val Loss: 7.6188 Acc: 0.0750\n",
      "\n",
      "Epoch 168/299\n",
      "----------\n",
      "train Loss: 0.8262 Acc: 0.7747\n",
      "val Loss: 8.1838 Acc: 0.0900\n",
      "\n",
      "Epoch 169/299\n",
      "----------\n",
      "train Loss: 0.9830 Acc: 0.7381\n",
      "val Loss: 8.1523 Acc: 0.0750\n",
      "\n",
      "Epoch 170/299\n",
      "----------\n",
      "train Loss: 0.9708 Acc: 0.7308\n",
      "val Loss: 7.6428 Acc: 0.0650\n",
      "\n",
      "Epoch 171/299\n",
      "----------\n",
      "train Loss: 0.9040 Acc: 0.7546\n",
      "val Loss: 7.8771 Acc: 0.0600\n",
      "\n",
      "Epoch 172/299\n",
      "----------\n",
      "train Loss: 1.0089 Acc: 0.7179\n",
      "val Loss: 7.7401 Acc: 0.0800\n",
      "\n",
      "Epoch 173/299\n",
      "----------\n",
      "train Loss: 0.9350 Acc: 0.7619\n",
      "val Loss: 7.9078 Acc: 0.0800\n",
      "\n",
      "Epoch 174/299\n",
      "----------\n",
      "train Loss: 1.0295 Acc: 0.7381\n",
      "val Loss: 8.1544 Acc: 0.0450\n",
      "\n",
      "Epoch 175/299\n",
      "----------\n",
      "train Loss: 0.9854 Acc: 0.7106\n",
      "val Loss: 7.7687 Acc: 0.0650\n",
      "\n",
      "Epoch 176/299\n",
      "----------\n",
      "train Loss: 0.9369 Acc: 0.7436\n",
      "val Loss: 8.5480 Acc: 0.0950\n",
      "\n",
      "Epoch 177/299\n",
      "----------\n",
      "train Loss: 0.9323 Acc: 0.7491\n",
      "val Loss: 7.5229 Acc: 0.0600\n",
      "\n",
      "Epoch 178/299\n",
      "----------\n",
      "train Loss: 0.8942 Acc: 0.7747\n",
      "val Loss: 7.8354 Acc: 0.0700\n",
      "\n",
      "Epoch 179/299\n",
      "----------\n",
      "train Loss: 0.8978 Acc: 0.7363\n",
      "val Loss: 7.8965 Acc: 0.0500\n",
      "\n",
      "Epoch 180/299\n",
      "----------\n",
      "train Loss: 0.8385 Acc: 0.7637\n",
      "val Loss: 7.6356 Acc: 0.0700\n",
      "\n",
      "Epoch 181/299\n",
      "----------\n",
      "train Loss: 0.8518 Acc: 0.7527\n",
      "val Loss: 7.5164 Acc: 0.0850\n",
      "\n",
      "Epoch 182/299\n",
      "----------\n",
      "train Loss: 0.9276 Acc: 0.7454\n",
      "val Loss: 7.8427 Acc: 0.0800\n",
      "\n",
      "Epoch 183/299\n",
      "----------\n",
      "train Loss: 0.9492 Acc: 0.7234\n",
      "val Loss: 7.4045 Acc: 0.0850\n",
      "\n",
      "Epoch 184/299\n",
      "----------\n",
      "train Loss: 0.8267 Acc: 0.7582\n",
      "val Loss: 7.8945 Acc: 0.0700\n",
      "\n",
      "Epoch 185/299\n",
      "----------\n",
      "train Loss: 0.8064 Acc: 0.7747\n",
      "val Loss: 7.8869 Acc: 0.0750\n",
      "\n",
      "Epoch 186/299\n",
      "----------\n",
      "train Loss: 0.7733 Acc: 0.8004\n",
      "val Loss: 8.2384 Acc: 0.0850\n",
      "\n",
      "Epoch 187/299\n",
      "----------\n",
      "train Loss: 0.8128 Acc: 0.7766\n",
      "val Loss: 7.9930 Acc: 0.0950\n",
      "\n",
      "Epoch 188/299\n",
      "----------\n",
      "train Loss: 0.7932 Acc: 0.7711\n",
      "val Loss: 7.9530 Acc: 0.0800\n",
      "\n",
      "Epoch 189/299\n",
      "----------\n",
      "train Loss: 0.8215 Acc: 0.7601\n",
      "val Loss: 7.8912 Acc: 0.0400\n",
      "\n",
      "Epoch 190/299\n",
      "----------\n",
      "train Loss: 0.8294 Acc: 0.7821\n",
      "val Loss: 7.5136 Acc: 0.0900\n",
      "\n",
      "Epoch 191/299\n",
      "----------\n",
      "train Loss: 0.7732 Acc: 0.7857\n",
      "val Loss: 7.5626 Acc: 0.0900\n",
      "\n",
      "Epoch 192/299\n",
      "----------\n",
      "train Loss: 0.7983 Acc: 0.7839\n",
      "val Loss: 7.8495 Acc: 0.0950\n",
      "\n",
      "Epoch 193/299\n",
      "----------\n",
      "train Loss: 0.8031 Acc: 0.7747\n",
      "val Loss: 8.5553 Acc: 0.0700\n",
      "\n",
      "Epoch 194/299\n",
      "----------\n",
      "train Loss: 0.8070 Acc: 0.7692\n",
      "val Loss: 8.0241 Acc: 0.0700\n",
      "\n",
      "Epoch 195/299\n",
      "----------\n",
      "train Loss: 0.7441 Acc: 0.7784\n",
      "val Loss: 8.3042 Acc: 0.0900\n",
      "\n",
      "Epoch 196/299\n",
      "----------\n",
      "train Loss: 0.8439 Acc: 0.7875\n",
      "val Loss: 8.5191 Acc: 0.0750\n",
      "\n",
      "Epoch 197/299\n",
      "----------\n",
      "train Loss: 0.7425 Acc: 0.7875\n",
      "val Loss: 8.0021 Acc: 0.0700\n",
      "\n",
      "Epoch 198/299\n",
      "----------\n",
      "train Loss: 0.7387 Acc: 0.7912\n",
      "val Loss: 7.6910 Acc: 0.0900\n",
      "\n",
      "Epoch 199/299\n",
      "----------\n",
      "train Loss: 0.7455 Acc: 0.8004\n",
      "val Loss: 8.6666 Acc: 0.0750\n",
      "\n",
      "Epoch 200/299\n",
      "----------\n",
      "train Loss: 0.7803 Acc: 0.7656\n",
      "val Loss: 8.1826 Acc: 0.0750\n",
      "\n",
      "Epoch 201/299\n",
      "----------\n",
      "train Loss: 0.8628 Acc: 0.7546\n",
      "val Loss: 8.4282 Acc: 0.0950\n",
      "\n",
      "Epoch 202/299\n",
      "----------\n",
      "train Loss: 0.6221 Acc: 0.8150\n",
      "val Loss: 8.2070 Acc: 0.0900\n",
      "\n",
      "Epoch 203/299\n",
      "----------\n",
      "train Loss: 0.9722 Acc: 0.7454\n",
      "val Loss: 8.8376 Acc: 0.0850\n",
      "\n",
      "Epoch 204/299\n",
      "----------\n",
      "train Loss: 0.8745 Acc: 0.7619\n",
      "val Loss: 8.0653 Acc: 0.0850\n",
      "\n",
      "Epoch 205/299\n",
      "----------\n",
      "train Loss: 0.9062 Acc: 0.7509\n",
      "val Loss: 8.9578 Acc: 0.0950\n",
      "\n",
      "Epoch 206/299\n",
      "----------\n",
      "train Loss: 0.8973 Acc: 0.7418\n",
      "val Loss: 8.3422 Acc: 0.0650\n",
      "\n",
      "Epoch 207/299\n",
      "----------\n",
      "train Loss: 0.7209 Acc: 0.7930\n",
      "val Loss: 7.9465 Acc: 0.0850\n",
      "\n",
      "Epoch 208/299\n",
      "----------\n",
      "train Loss: 0.6954 Acc: 0.8297\n",
      "val Loss: 8.2357 Acc: 0.0500\n",
      "\n",
      "Epoch 209/299\n",
      "----------\n",
      "train Loss: 0.6739 Acc: 0.8132\n",
      "val Loss: 8.1144 Acc: 0.0800\n",
      "\n",
      "Epoch 210/299\n",
      "----------\n",
      "train Loss: 0.7513 Acc: 0.7912\n",
      "val Loss: 8.5497 Acc: 0.0800\n",
      "\n",
      "Epoch 211/299\n",
      "----------\n",
      "train Loss: 0.7227 Acc: 0.7949\n",
      "val Loss: 8.1233 Acc: 0.0800\n",
      "\n",
      "Epoch 212/299\n",
      "----------\n",
      "train Loss: 0.7145 Acc: 0.8114\n",
      "val Loss: 7.9699 Acc: 0.0750\n",
      "\n",
      "Epoch 213/299\n",
      "----------\n",
      "train Loss: 0.7031 Acc: 0.8242\n",
      "val Loss: 8.3634 Acc: 0.0750\n",
      "\n",
      "Epoch 214/299\n",
      "----------\n",
      "train Loss: 0.7266 Acc: 0.8095\n",
      "val Loss: 8.3966 Acc: 0.0700\n",
      "\n",
      "Epoch 215/299\n",
      "----------\n",
      "train Loss: 0.7090 Acc: 0.7967\n",
      "val Loss: 8.4842 Acc: 0.0600\n",
      "\n",
      "Epoch 216/299\n",
      "----------\n",
      "train Loss: 0.7017 Acc: 0.7930\n",
      "val Loss: 8.3762 Acc: 0.0700\n",
      "\n",
      "Epoch 217/299\n",
      "----------\n",
      "train Loss: 0.7179 Acc: 0.7949\n",
      "val Loss: 8.1931 Acc: 0.0600\n",
      "\n",
      "Epoch 218/299\n",
      "----------\n",
      "train Loss: 0.6901 Acc: 0.8004\n",
      "val Loss: 8.6473 Acc: 0.1000\n",
      "\n",
      "Epoch 219/299\n",
      "----------\n",
      "train Loss: 0.7593 Acc: 0.8022\n",
      "val Loss: 8.3306 Acc: 0.0700\n",
      "\n",
      "Epoch 220/299\n",
      "----------\n",
      "train Loss: 0.7209 Acc: 0.8095\n",
      "val Loss: 8.0754 Acc: 0.0950\n",
      "\n",
      "Epoch 221/299\n",
      "----------\n",
      "train Loss: 0.7585 Acc: 0.7949\n",
      "val Loss: 8.2599 Acc: 0.0700\n",
      "\n",
      "Epoch 222/299\n",
      "----------\n",
      "train Loss: 0.6701 Acc: 0.8095\n",
      "val Loss: 8.0320 Acc: 0.0750\n",
      "\n",
      "Epoch 223/299\n",
      "----------\n",
      "train Loss: 0.7263 Acc: 0.8059\n",
      "val Loss: 7.9302 Acc: 0.0750\n",
      "\n",
      "Epoch 224/299\n",
      "----------\n",
      "train Loss: 0.7305 Acc: 0.8095\n",
      "val Loss: 8.1072 Acc: 0.0650\n",
      "\n",
      "Epoch 225/299\n",
      "----------\n",
      "train Loss: 0.6959 Acc: 0.7985\n",
      "val Loss: 8.0358 Acc: 0.1000\n",
      "\n",
      "Epoch 226/299\n",
      "----------\n",
      "train Loss: 0.7772 Acc: 0.7857\n",
      "val Loss: 8.3577 Acc: 0.0750\n",
      "\n",
      "Epoch 227/299\n",
      "----------\n",
      "train Loss: 0.7026 Acc: 0.8297\n",
      "val Loss: 8.9224 Acc: 0.0650\n",
      "\n",
      "Epoch 228/299\n",
      "----------\n",
      "train Loss: 0.5024 Acc: 0.8681\n",
      "val Loss: 8.5211 Acc: 0.0750\n",
      "\n",
      "Epoch 229/299\n",
      "----------\n",
      "train Loss: 0.7173 Acc: 0.8132\n",
      "val Loss: 8.1671 Acc: 0.0650\n",
      "\n",
      "Epoch 230/299\n",
      "----------\n",
      "train Loss: 0.6328 Acc: 0.8297\n",
      "val Loss: 8.3610 Acc: 0.0750\n",
      "\n",
      "Epoch 231/299\n",
      "----------\n",
      "train Loss: 0.6796 Acc: 0.8150\n",
      "val Loss: 8.3227 Acc: 0.0850\n",
      "\n",
      "Epoch 232/299\n",
      "----------\n",
      "train Loss: 0.7234 Acc: 0.7912\n",
      "val Loss: 8.1425 Acc: 0.0950\n",
      "\n",
      "Epoch 233/299\n",
      "----------\n",
      "train Loss: 0.6553 Acc: 0.8205\n",
      "val Loss: 8.1491 Acc: 0.0800\n",
      "\n",
      "Epoch 234/299\n",
      "----------\n",
      "train Loss: 0.6356 Acc: 0.8242\n",
      "val Loss: 8.2851 Acc: 0.0900\n",
      "\n",
      "Epoch 235/299\n",
      "----------\n",
      "train Loss: 0.7083 Acc: 0.8132\n",
      "val Loss: 8.0121 Acc: 0.0750\n",
      "\n",
      "Epoch 236/299\n",
      "----------\n",
      "train Loss: 0.7144 Acc: 0.7967\n",
      "val Loss: 8.4439 Acc: 0.0600\n",
      "\n",
      "Epoch 237/299\n",
      "----------\n",
      "train Loss: 0.6108 Acc: 0.8205\n",
      "val Loss: 7.9877 Acc: 0.0800\n",
      "\n",
      "Epoch 238/299\n",
      "----------\n",
      "train Loss: 0.6592 Acc: 0.8132\n",
      "val Loss: 8.2625 Acc: 0.0700\n",
      "\n",
      "Epoch 239/299\n",
      "----------\n",
      "train Loss: 0.7135 Acc: 0.8022\n",
      "val Loss: 8.1481 Acc: 0.0900\n",
      "\n",
      "Epoch 240/299\n",
      "----------\n",
      "train Loss: 0.7410 Acc: 0.7839\n",
      "val Loss: 8.4984 Acc: 0.0700\n",
      "\n",
      "Epoch 241/299\n",
      "----------\n",
      "train Loss: 0.6958 Acc: 0.8022\n",
      "val Loss: 7.9543 Acc: 0.0750\n",
      "\n",
      "Epoch 242/299\n",
      "----------\n",
      "train Loss: 0.7703 Acc: 0.7894\n",
      "val Loss: 8.0755 Acc: 0.0750\n",
      "\n",
      "Epoch 243/299\n",
      "----------\n",
      "train Loss: 0.6971 Acc: 0.8040\n",
      "val Loss: 8.0838 Acc: 0.0800\n",
      "\n",
      "Epoch 244/299\n",
      "----------\n",
      "train Loss: 0.6689 Acc: 0.8059\n",
      "val Loss: 8.6085 Acc: 0.0800\n",
      "\n",
      "Epoch 245/299\n",
      "----------\n",
      "train Loss: 0.5828 Acc: 0.8388\n",
      "val Loss: 7.9596 Acc: 0.0750\n",
      "\n",
      "Epoch 246/299\n",
      "----------\n",
      "train Loss: 0.5355 Acc: 0.8443\n",
      "val Loss: 8.5301 Acc: 0.0700\n",
      "\n",
      "Epoch 247/299\n",
      "----------\n",
      "train Loss: 0.7017 Acc: 0.8114\n",
      "val Loss: 8.2411 Acc: 0.0650\n",
      "\n",
      "Epoch 248/299\n",
      "----------\n",
      "train Loss: 0.6614 Acc: 0.8223\n",
      "val Loss: 8.6925 Acc: 0.0550\n",
      "\n",
      "Epoch 249/299\n",
      "----------\n",
      "train Loss: 0.6455 Acc: 0.8168\n",
      "val Loss: 7.9616 Acc: 0.0750\n",
      "\n",
      "Epoch 250/299\n",
      "----------\n",
      "train Loss: 0.5135 Acc: 0.8700\n",
      "val Loss: 8.3018 Acc: 0.0750\n",
      "\n",
      "Epoch 251/299\n",
      "----------\n",
      "train Loss: 0.5528 Acc: 0.8333\n",
      "val Loss: 8.6914 Acc: 0.0800\n",
      "\n",
      "Epoch 252/299\n",
      "----------\n",
      "train Loss: 0.5252 Acc: 0.8755\n",
      "val Loss: 8.6985 Acc: 0.0950\n",
      "\n",
      "Epoch 253/299\n",
      "----------\n",
      "train Loss: 0.5264 Acc: 0.8443\n",
      "val Loss: 8.3220 Acc: 0.0750\n",
      "\n",
      "Epoch 254/299\n",
      "----------\n",
      "train Loss: 0.5526 Acc: 0.8370\n",
      "val Loss: 8.5156 Acc: 0.0950\n",
      "\n",
      "Epoch 255/299\n",
      "----------\n",
      "train Loss: 0.6398 Acc: 0.8187\n",
      "val Loss: 8.7601 Acc: 0.0850\n",
      "\n",
      "Epoch 256/299\n",
      "----------\n",
      "train Loss: 0.5515 Acc: 0.8278\n",
      "val Loss: 8.8218 Acc: 0.0750\n",
      "\n",
      "Epoch 257/299\n",
      "----------\n",
      "train Loss: 0.5976 Acc: 0.8242\n",
      "val Loss: 8.3981 Acc: 0.0900\n",
      "\n",
      "Epoch 258/299\n",
      "----------\n",
      "train Loss: 0.5768 Acc: 0.8388\n",
      "val Loss: 8.6195 Acc: 0.0950\n",
      "\n",
      "Epoch 259/299\n",
      "----------\n",
      "train Loss: 0.5634 Acc: 0.8205\n",
      "val Loss: 8.5234 Acc: 0.1000\n",
      "\n",
      "Epoch 260/299\n",
      "----------\n",
      "train Loss: 0.5673 Acc: 0.8370\n",
      "val Loss: 9.0285 Acc: 0.0800\n",
      "\n",
      "Epoch 261/299\n",
      "----------\n",
      "train Loss: 0.6024 Acc: 0.8223\n",
      "val Loss: 8.1102 Acc: 0.0900\n",
      "\n",
      "Epoch 262/299\n",
      "----------\n",
      "train Loss: 0.5447 Acc: 0.8626\n",
      "val Loss: 8.8162 Acc: 0.0950\n",
      "\n",
      "Epoch 263/299\n",
      "----------\n",
      "train Loss: 0.6920 Acc: 0.8205\n",
      "val Loss: 8.6589 Acc: 0.0750\n",
      "\n",
      "Epoch 264/299\n",
      "----------\n",
      "train Loss: 0.7141 Acc: 0.8004\n",
      "val Loss: 9.0491 Acc: 0.0550\n",
      "\n",
      "Epoch 265/299\n",
      "----------\n",
      "train Loss: 0.6328 Acc: 0.8168\n",
      "val Loss: 8.5531 Acc: 0.0750\n",
      "\n",
      "Epoch 266/299\n",
      "----------\n",
      "train Loss: 1.0046 Acc: 0.7216\n",
      "val Loss: 9.2447 Acc: 0.0500\n",
      "\n",
      "Epoch 267/299\n",
      "----------\n",
      "train Loss: 0.7980 Acc: 0.7747\n",
      "val Loss: 8.2818 Acc: 0.0700\n",
      "\n",
      "Epoch 268/299\n",
      "----------\n",
      "train Loss: 0.6748 Acc: 0.8132\n",
      "val Loss: 8.7413 Acc: 0.0900\n",
      "\n",
      "Epoch 269/299\n",
      "----------\n",
      "train Loss: 0.5786 Acc: 0.8462\n",
      "val Loss: 8.4367 Acc: 0.1200\n",
      "\n",
      "Epoch 270/299\n",
      "----------\n",
      "train Loss: 0.6733 Acc: 0.8040\n",
      "val Loss: 8.7219 Acc: 0.0850\n",
      "\n",
      "Epoch 271/299\n",
      "----------\n",
      "train Loss: 0.6944 Acc: 0.7875\n",
      "val Loss: 8.4033 Acc: 0.0800\n",
      "\n",
      "Epoch 272/299\n",
      "----------\n",
      "train Loss: 0.7464 Acc: 0.7930\n",
      "val Loss: 8.8952 Acc: 0.1100\n",
      "\n",
      "Epoch 273/299\n",
      "----------\n",
      "train Loss: 0.6785 Acc: 0.7894\n",
      "val Loss: 8.5882 Acc: 0.0750\n",
      "\n",
      "Epoch 274/299\n",
      "----------\n",
      "train Loss: 0.8657 Acc: 0.7637\n",
      "val Loss: 8.6331 Acc: 0.0800\n",
      "\n",
      "Epoch 275/299\n",
      "----------\n",
      "train Loss: 0.7489 Acc: 0.7674\n",
      "val Loss: 8.2543 Acc: 0.0600\n",
      "\n",
      "Epoch 276/299\n",
      "----------\n",
      "train Loss: 0.5139 Acc: 0.8516\n",
      "val Loss: 8.4330 Acc: 0.0750\n",
      "\n",
      "Epoch 277/299\n",
      "----------\n",
      "train Loss: 0.5166 Acc: 0.8462\n",
      "val Loss: 8.5936 Acc: 0.0700\n",
      "\n",
      "Epoch 278/299\n",
      "----------\n",
      "train Loss: 0.6129 Acc: 0.8333\n",
      "val Loss: 8.2648 Acc: 0.0850\n",
      "\n",
      "Epoch 279/299\n",
      "----------\n",
      "train Loss: 0.6425 Acc: 0.8077\n",
      "val Loss: 8.3377 Acc: 0.0800\n",
      "\n",
      "Epoch 280/299\n",
      "----------\n",
      "train Loss: 0.7619 Acc: 0.7875\n",
      "val Loss: 8.2879 Acc: 0.0700\n",
      "\n",
      "Epoch 281/299\n",
      "----------\n",
      "train Loss: 0.6826 Acc: 0.8315\n",
      "val Loss: 8.7663 Acc: 0.0650\n",
      "\n",
      "Epoch 282/299\n",
      "----------\n",
      "train Loss: 0.5698 Acc: 0.8443\n",
      "val Loss: 8.8067 Acc: 0.0450\n",
      "\n",
      "Epoch 283/299\n",
      "----------\n",
      "train Loss: 0.6498 Acc: 0.8242\n",
      "val Loss: 8.7292 Acc: 0.0750\n",
      "\n",
      "Epoch 284/299\n",
      "----------\n",
      "train Loss: 0.5856 Acc: 0.8223\n",
      "val Loss: 8.3633 Acc: 0.0750\n",
      "\n",
      "Epoch 285/299\n",
      "----------\n",
      "train Loss: 0.6338 Acc: 0.8260\n",
      "val Loss: 8.6972 Acc: 0.0650\n",
      "\n",
      "Epoch 286/299\n",
      "----------\n",
      "train Loss: 0.6295 Acc: 0.8407\n",
      "val Loss: 8.4754 Acc: 0.0650\n",
      "\n",
      "Epoch 287/299\n",
      "----------\n",
      "train Loss: 0.5684 Acc: 0.8260\n",
      "val Loss: 8.7302 Acc: 0.0750\n",
      "\n",
      "Epoch 288/299\n",
      "----------\n",
      "train Loss: 0.6441 Acc: 0.8297\n",
      "val Loss: 8.7253 Acc: 0.0800\n",
      "\n",
      "Epoch 289/299\n",
      "----------\n",
      "train Loss: 0.5831 Acc: 0.8388\n",
      "val Loss: 8.3331 Acc: 0.0950\n",
      "\n",
      "Epoch 290/299\n",
      "----------\n",
      "train Loss: 0.6732 Acc: 0.8187\n",
      "val Loss: 8.4593 Acc: 0.0650\n",
      "\n",
      "Epoch 291/299\n",
      "----------\n",
      "train Loss: 0.5484 Acc: 0.8443\n",
      "val Loss: 9.0053 Acc: 0.0750\n",
      "\n",
      "Epoch 292/299\n",
      "----------\n",
      "train Loss: 0.6184 Acc: 0.8297\n",
      "val Loss: 8.8227 Acc: 0.0800\n",
      "\n",
      "Epoch 293/299\n",
      "----------\n",
      "train Loss: 0.5495 Acc: 0.8480\n",
      "val Loss: 8.9052 Acc: 0.0900\n",
      "\n",
      "Epoch 294/299\n",
      "----------\n",
      "train Loss: 0.6314 Acc: 0.8187\n",
      "val Loss: 8.5643 Acc: 0.0800\n",
      "\n",
      "Epoch 295/299\n",
      "----------\n",
      "train Loss: 0.5123 Acc: 0.8681\n",
      "val Loss: 8.4949 Acc: 0.0750\n",
      "\n",
      "Epoch 296/299\n",
      "----------\n",
      "train Loss: 0.6475 Acc: 0.8205\n",
      "val Loss: 8.2315 Acc: 0.0900\n",
      "\n",
      "Epoch 297/299\n",
      "----------\n",
      "train Loss: 0.5909 Acc: 0.8278\n",
      "val Loss: 8.2582 Acc: 0.0550\n",
      "\n",
      "Epoch 298/299\n",
      "----------\n",
      "train Loss: 0.6700 Acc: 0.8059\n",
      "val Loss: 8.1812 Acc: 0.0750\n",
      "\n",
      "Epoch 299/299\n",
      "----------\n",
      "train Loss: 0.5114 Acc: 0.8590\n",
      "val Loss: 8.6462 Acc: 0.0650\n",
      "\n",
      "Training complete in 241m 36s\n",
      "Best val Acc: 0.120000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['hist_resnet34_to_e_300_b_32.joblib']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup the loss fxn\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train and evaluate\n",
    "model_ft, train_hist, val_hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs)\n",
    "torch.save(model_ft.state_dict(), f\"{model_name}_{data_dir[7:9]}_e_{num_epochs}_b_{batch_size}.pth\")\n",
    "\n",
    "joblib.dump([train_hist, val_hist], f\"hist_{model_name}_{data_dir[7:9]}_e_{num_epochs}_b_{batch_size}.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert hists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib as joblib\n",
    "import numpy as np\n",
    "model_names = []\n",
    "#model_names = [\n",
    "    # 'hist_resnet18_to_e_300_b_32', 'hist_resnet34_to_e_300_b_32', 'hist_densenet_to_e_300_b_32',\n",
    "      #      'hist_alexnet_to_e_300_b_32',\n",
    "#            'hist_squeezenet_to_e_300_b_32',\n",
    "       #     'hist_vgg_to_e_300_b_32'\n",
    "#       ]\n",
    "\n",
    "for name in model_names:\n",
    "    print(f\"Converting '{name}.joblib'\")\n",
    "    train_res_new = []\n",
    "    val_res_new = []\n",
    "    [train_res, val_res] = joblib.load(f\"{name}.joblib\")\n",
    "    rnge=len(train_res)\n",
    "    for i in range(rnge):\n",
    "        train_res_new.append(train_res[i])\n",
    "        val_res_new.append(val_res[i].item())\n",
    "    joblib.dump([train_res_new, val_res_new], f\"{name}.joblib\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "78010e648cab4d2ba8c0f9090d51258d09f88d036db090ce2c604fe3a9dbf2ab"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
